{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064605af",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097b456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-api-python-client\n",
      "  Obtaining dependency information for google-api-python-client from https://files.pythonhosted.org/packages/8f/a7/817a0fc24cf948edf3ce1e3220f82d82eccbe3f96d50eba89392c8d673dd/google_api_python_client-2.141.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_api_python_client-2.141.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for httplib2<1.dev0,>=0.19.0 from https://files.pythonhosted.org/packages/a8/6c/d2fbdaaa5959339d53ba38e94c123e4e84b8fbc4b84beb0e70d7c1608486/httplib2-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 from https://files.pythonhosted.org/packages/60/57/0f37c6f35847e26b7bea7d5e4f069cf037fd792cf8b67206311761e7bb92/google_auth-2.33.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth-httplib2<1.0.0,>=0.2.0 from https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 from https://files.pythonhosted.org/packages/44/99/daa3541e8ecd7d8b7907b714ba92126097a976b5b3dbabdb5febdcf08554/google_api_core-2.19.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Obtaining dependency information for uritemplate<5,>=3.0.1 from https://files.pythonhosted.org/packages/81/c0/7461b49cd25aeece13766f02ee576d1db528f1c37ce69aee300e075b485b/uritemplate-4.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/02/48/87422ff1bddcae677fb6f58c97f5cfc613304a5e8ce2c3662760199c0a84/googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 from https://files.pythonhosted.org/packages/a4/30/cb5395acd5f65edc0dee77bdd134fe556c52fade2ad3ea9ac2676d01effe/protobuf-5.27.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for proto-plus<2.0.0dev,>=1.22.3 from https://files.pythonhosted.org/packages/7c/6f/db31f0711c0402aa477257205ce7d29e86a75cb52cd19f7afb585f75cda0/proto_plus-1.24.0-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/04/e6/a1551acbaa06f3e48b311329828a34bc9c51a8cfaecdeb4d03c329a1ef85/cachetools-5.4.0-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Downloading google_api_python_client-2.141.0-py2.py3-none-any.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.2 MB 660.6 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.0/12.2 MB 326.8 kB/s eta 0:00:38\n",
      "   ---------------------------------------- 0.1/12.2 MB 656.4 kB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.1/12.2 MB 708.1 kB/s eta 0:00:17\n",
      "    --------------------------------------- 0.2/12.2 MB 787.7 kB/s eta 0:00:16\n",
      "    --------------------------------------- 0.2/12.2 MB 765.3 kB/s eta 0:00:16\n",
      "    --------------------------------------- 0.3/12.2 MB 749.3 kB/s eta 0:00:16\n",
      "    --------------------------------------- 0.3/12.2 MB 707.1 kB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.3/12.2 MB 703.7 kB/s eta 0:00:17\n",
      "   - -------------------------------------- 0.3/12.2 MB 678.1 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/12.2 MB 678.1 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/12.2 MB 678.1 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.3/12.2 MB 678.1 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.6/12.2 MB 910.6 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.6/12.2 MB 910.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.6/12.2 MB 805.5 kB/s eta 0:00:15\n",
      "   -- ------------------------------------- 0.7/12.2 MB 848.0 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.7/12.2 MB 873.6 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.2 MB 907.9 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 0.9/12.2 MB 971.6 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/12.2 MB 978.2 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.1/12.2 MB 1.0 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.2/12.2 MB 1.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.2/12.2 MB 1.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.3/12.2 MB 1.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.4/12.2 MB 1.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.4/12.2 MB 1.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.5/12.2 MB 1.2 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.7/12.2 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.8/12.2 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.9/12.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.0/12.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.0/12.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.1/12.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.2/12.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.3/12.2 MB 1.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.4/12.2 MB 1.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.5/12.2 MB 1.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.6/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.7/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.8/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.9/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.0/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.2/12.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 3.3/12.2 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.5/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.6/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.7/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.8/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.0/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.1/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.3/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.3/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.4/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.4/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.5/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.6/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.7/12.2 MB 1.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.7/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.8/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.9/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.1/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.3/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.4/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.5/12.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.7/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.8/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 5.9/12.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.0/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.2/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.2/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.3/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.4/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.5/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.6/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.7/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.2/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.2/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.3/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.4/12.2 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.5/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.5/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.6/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.7/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.0/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.2/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.2/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.3/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.4/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.4/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.5/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.6/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.6/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.7/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.8/12.2 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.8/12.2 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.0/12.2 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.1/12.2 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.2/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.3/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.3/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.4/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.4/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.5/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.6/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.6/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.7/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.8/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.9/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.9/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.0/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.3/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.3/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.4/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.4/12.2 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.6/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.6/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.9/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.9/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.2/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.2/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/139.4 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 41.0/139.4 kB 487.6 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 61.4/139.4 kB 544.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------- 92.2/139.4 kB 525.1 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 112.6/139.4 kB 544.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 139.4/139.4 kB 516.6 kB/s eta 0:00:00\n",
      "Downloading google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "   ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/200.5 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 41.0/200.5 kB 487.6 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 61.4/200.5 kB 544.7 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 92.2/200.5 kB 525.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 112.6/200.5 kB 544.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 143.4/200.5 kB 532.5 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 163.8/200.5 kB 546.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 200.5/200.5 kB 553.2 kB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/96.9 kB 660.6 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 41.0/96.9 kB 653.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/96.9 kB 573.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 96.9/96.9 kB 555.2 kB/s eta 0:00:00\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/220.0 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 41.0/220.0 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 61.4/220.0 kB 656.4 kB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 92.2/220.0 kB 655.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 122.9/220.0 kB 554.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 143.4/220.0 kB 607.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 174.1/220.0 kB 583.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 204.8/220.0 kB 593.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 220.0/220.0 kB 584.3 kB/s eta 0:00:00\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 30.7/50.1 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 50.1/50.1 kB 847.7 kB/s eta 0:00:00\n",
      "Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl (426 kB)\n",
      "   ---------------------------------------- 0.0/426.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/426.9 kB 660.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/426.9 kB 653.6 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 81.9/426.9 kB 508.4 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 112.6/426.9 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 143.4/426.9 kB 568.9 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 174.1/426.9 kB 615.9 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 194.6/426.9 kB 588.9 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 225.3/426.9 kB 599.0 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 235.5/426.9 kB 554.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/426.9 kB 607.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 307.2/426.9 kB 593.9 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 337.9/426.9 kB 599.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/426.9 kB 603.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 389.1/426.9 kB 606.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 426.9/426.9 kB 605.7 kB/s eta 0:00:00\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, cachetools, proto-plus, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.4.0 google-api-core-2.19.1 google-api-python-client-2.141.0 google-auth-2.33.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.63.2 httplib2-0.22.0 proto-plus-1.24.0 protobuf-5.27.3 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\ASHIR-MEHMOOD\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c3a6d",
   "metadata": {},
   "source": [
    "# Fetching Comments from Yutube using Goolgle Cloud API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb4f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments from video: tZ3D4I7aSww\n",
      "Fetching comments from video: 2_mWEL0Nuk8\n",
      "Fetching comments from video: MjxGwfa5lxw\n",
      "Fetching comments from video: Xb_pvujKCPE\n",
      "Total comments fetched: 94409\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import csv\n",
    "\n",
    "# API key from Google Cloud Console\n",
    "api_key = 'Your Own Api'  ]\n",
    "\n",
    "# Set up the YouTube API client\n",
    "youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Request comments using YouTube Data API\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            pageToken=next_page_token,\n",
    "            maxResults=100  # Number of comments to fetch per request\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract comments from the API response\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "            comments.append(comment)\n",
    "\n",
    "        # Check if there is another page of comments\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "def fetch_comments_from_multiple_videos(video_ids):\n",
    "    \"\"\"Fetch comments from multiple YouTube videos.\"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        print(f\"Fetching comments from video: {video_id}\")\n",
    "        comments = get_comments(video_id)\n",
    "        all_comments.extend(comments)\n",
    "    \n",
    "    return all_comments\n",
    "\n",
    "def save_comments_to_csv(comments, filename):\n",
    "    \"\"\"Save comments to a CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        comment_writer = csv.writer(csvfile)\n",
    "        comment_writer.writerow(['Comment'])  # Header\n",
    "        for comment in comments:\n",
    "            comment_writer.writerow([comment])\n",
    "\n",
    "# Extracted video IDs from video Links\n",
    "video_ids = [\n",
    "    'tZ3D4I7aSww',\n",
    "    '2_mWEL0Nuk8',\n",
    "    'MjxGwfa5lxw',\n",
    "    'Xb_pvujKCPE'\n",
    "]\n",
    "\n",
    "# Fetch comments\n",
    "all_comments = fetch_comments_from_multiple_videos(video_ids)\n",
    "\n",
    "# Saving comments to a CSV file\n",
    "save_comments_to_csv(all_comments, 'youtube_comments.csv')\n",
    "\n",
    "print(f\"Total comments fetched: {len(all_comments)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ea289",
   "metadata": {},
   "source": [
    "# Importing All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06cc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e25b5",
   "metadata": {},
   "source": [
    "# Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33623c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/Data/youtube_comments.csv'\n",
    "comments_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ba431",
   "metadata": {},
   "source": [
    "# Display dataset shape and a preview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb739ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (94409, 1)\n",
      "                                             Comment\n",
      "0    I love Pakistan Army from the core of my heart.\n",
      "1                               Operation hinterland\n",
      "2  Aik He Cherah hai jesay Dekh Muje Khud Bakhud ...\n",
      "3                                    I love pak army\n",
      "4                             حسبنا الله ونعم الوكيل\n",
      "5              الحمدلله پاکستان الله کی عظیم نعمت ھے\n",
      "6  Nafrt ho gai hain ap sy. Kya bna dea Ic mulk P...\n",
      "7         Israili army ke liye kam krti hai lol army\n",
      "      Comment\n",
      "94402    ❤❤❤❤\n",
      "94403   First\n",
      "94404   ❤❤❤❤❤\n",
      "94405   First\n",
      "94406      ❤❤\n",
      "94407       1\n",
      "94408   First\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the dataset is: {comments_df.shape}\")\n",
    "print(comments_df.head(8))\n",
    "print(comments_df.tail(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc0ea3",
   "metadata": {},
   "source": [
    "# Define Roman Urdu stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3a0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_urdu_stopwords = [\n",
    "    'aur', 'hai', 'ka', 'ke', 'ki', 'ko', 'main', 'mein', 'ne', 'se', 'to', 'wo',\n",
    "    'ye', 'ho', 'bhi', 'par', 'nahi', 'kar', 'raha', 'rahi', 'tha', 'the', 'is', 'us',\n",
    "    'mein', 'tum', 'yeh', 'woh', 'ap', 'apna', 'apne', 'mera', 'meri', 'mere', 'tumhara',\n",
    "    'hum', 'kya', 'kon', 'kis', 'kab', 'kaha', 'kyun', 'jab', 'tab', 'ab', 'bas', 'tak',\n",
    "    'hain', 'kuch', 'sab', 'jese', 'lekin', 'jabtak', 'magar', 'phir', 'kehti', 'kehte',\n",
    "    'rakh', 'kiya', 'gi', 'ge', 'kitna', 'kitni', 'aise', 'jese', 'janab', 'zara', 'abhi',\n",
    "    'ager', 'yahan', 'wahan', 'poora', 'isliye', 'kaun', 'karna', 'karti', 'karte', 'karta',\n",
    "    'karti', 'un', 'in', 'tumhein', 'mujhe', 'tumse', 'mujhse', 'inka', 'unka', 'unki', \n",
    "    'inki', 'uski', 'iski', 'unka', 'inka', 'unka', 'lekin', 'magar', 'kisi', 'aur',\n",
    "    'wah', 'yah', 'jana', 'ayega', 'ayegi', 'jaega', 'jaegi', 'acha', 'achi', 'hota',\n",
    "    'hoti', 'hona', 'zaroor', 'bilkul', 'kitni', 'zyada', 'bohot', 'itna', 'kam', 'acha',\n",
    "    'rahe', 'rehte', 'jati', 'jata', 'jatay', 'chal', 'chalti', 'chalte', 'raho', 'rakhna',\n",
    "    'yehi', 'is', 'aise', 'aise', 'baat', 'kuchh', 'aise', 'koji', 'wala', 'wale', 'wali',\n",
    "    'valay', 'jawaab', 'waqt', 'jaisa', 'jaisi', 'kaisa', 'kaisi', 'kaam', 'hona', 'aesa',\n",
    "    'sabse', 'sabko', 'sabhi', 'kuchh', 'saath', 'kisne', 'kis', 'kisi', 'isme', 'ismein',\n",
    "    'unka', 'unka', 'wahi', 'yahi', 'wahan', 'yahan', 'kabhi', 'kaun', 'dekho', 'dekhte',\n",
    "    'batao', 'bata', 'pata', 'se', 'kyun', 'koi', 'kuch', 'hamari', 'hamara', 'hamare',\n",
    "    'karlo', 'karun', 'chahie', 'laga', 'laga', 'laga', 'shuru', 'rahne', 'rahna', 'karna',\n",
    "    'kartay', 'karni', 'chuki', 'rehne', 'na', 'nahi', 'hai', 'hain'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227891d",
   "metadata": {},
   "source": [
    "# Save Roman Urdu stopwords to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d478675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roman Urdu stopwords have been saved to 'roman_urdu_stopwords.csv'.\n"
     ]
    }
   ],
   "source": [
    "stopwords_df = pd.DataFrame(roman_urdu_stopwords, columns=['Stopword'])\n",
    "stopwords_df.to_csv('C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/roman_urdu_stopwords.csv', index=False)\n",
    "print(\"Roman Urdu stopwords have been saved to 'roman_urdu_stopwords.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cde056",
   "metadata": {},
   "source": [
    "# Load English stopwords and Creating a Merged datafame of Roman Urdu and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88fab18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "roman_urdu_stopwords = pd.read_csv('C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/roman_urdu_stopwords.csv')\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "combined_stopwords = set(english_stopwords).union(set(roman_urdu_stopwords['Stopword']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e592e38",
   "metadata": {},
   "source": [
    "# Function to clean text by removing special characters, signs and Stopwords & Function to check if a comment is entirely in English &  Function to detect and filter out comments containing non-Latin characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bc7a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(comment):\n",
    "    \"\"\"Preprocess the comment by removing special characters, converting to lowercase, and standardizing spaces.\"\"\"\n",
    "    comment = re.sub(r'[^a-zA-Z0-9\\s]', '', comment)  # Remove special characters\n",
    "    comment = comment.lower()  # Convert to lowercase\n",
    "    comment = re.sub(r'\\s+', ' ', comment).strip()  # Remove extra spaces and strip leading/trailing spaces\n",
    "    return comment\n",
    "\n",
    "def remove_stopwords(comment):\n",
    "    \"\"\"Remove stopwords from the comment.\"\"\"\n",
    "    return ' '.join([word for word in comment.split() if word not in combined_stopwords])\n",
    "\n",
    "def is_english_comment(comment):\n",
    "    \"\"\"Check if a comment is entirely in English.\"\"\"\n",
    "    words = comment.split()\n",
    "    if len(words) == 0:  # If the comment is empty, return False\n",
    "        return False\n",
    "    num_english_words = sum(1 for word in words if word.lower() in english_stopwords)\n",
    "    # Checking if the proportion of English words is above 90% or all words are English\n",
    "    return num_english_words == len(words) or num_english_words / len(words) > 0.9\n",
    "\n",
    "\n",
    "def contains_non_latin(comment):\n",
    "    \"\"\"Detect and filter out comments containing non-Latin characters.\"\"\"\n",
    "    non_latin_pattern = re.compile(r'[^\\u0000-\\u007F]+')\n",
    "    return bool(non_latin_pattern.search(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc65c69",
   "metadata": {},
   "source": [
    "# 2. Pre- Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e933d47",
   "metadata": {},
   "source": [
    "1. Fill NaN values with empty strings, clean text, remove stopwords, remove duplicates\n",
    "2. Remove empty lines\n",
    "3. Remove complete english scripts & Filter out comments containing non-Latin characters\n",
    "4. Filter Noise and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3909ef",
   "metadata": {},
   "source": [
    "Preprocess comments: fill NaN values with empty strings , clean text, convert to lowercase, and standardize spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2304fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['Comment'] = comments_df['Comment'].fillna('') \n",
    "comments_df['Cleaned_Comment'] = comments_df['Comment'].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600fa1d",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c071352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['Cleaned_Comment'] = comments_df['Cleaned_Comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5411f0f",
   "metadata": {},
   "source": [
    "Remove duplicate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a27dc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop_duplicates(subset='Cleaned_Comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cc344",
   "metadata": {},
   "source": [
    "Remove comments that are entirely in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1298884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df[~comments_df['Cleaned_Comment'].apply(is_english_comment)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273a4a7",
   "metadata": {},
   "source": [
    "Filter out comments containing non-Latin characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f3ee6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df[~comments_df['Cleaned_Comment'].apply(contains_non_latin)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954ef6f",
   "metadata": {},
   "source": [
    "Remove empty lines (comments that are completely blank or contain only whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d58d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df[comments_df['Cleaned_Comment'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b92745",
   "metadata": {},
   "source": [
    "Remove comments with very short length or repetitive characters\n",
    "\n",
    "regex pattern to find sequences of repeated characters (three or more of the same character)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "542f32ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5464\\2538059551.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  comments_df = comments_df[~comments_df['Cleaned_Comment'].str.contains(r'(.)\\1{2,}', regex=True)]\n"
     ]
    }
   ],
   "source": [
    "comments_df = comments_df[comments_df['Cleaned_Comment'].str.len() > 2]  # Example threshold: length > 2\n",
    "\n",
    "comments_df = comments_df[~comments_df['Cleaned_Comment'].str.contains(r'(.)\\1{2,}', regex=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557ef2b",
   "metadata": {},
   "source": [
    " Remove very short and very long comments \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f44d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df[comments_df['Cleaned_Comment'].str.len() > 5]  # Example: keep comments longer than 5 characters\n",
    "\n",
    "comments_df = comments_df[comments_df['Cleaned_Comment'].str.len() < 500]  # Example: remove comments longer than 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39998ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.rename(columns={'Cleaned_Comment': 'Roman Urdu Comments'}, inplace=True)\n",
    "comments_df['Toxic comments'] = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca871f48",
   "metadata": {},
   "source": [
    "# Display the shape and a preview of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff3c110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates, stopwords, English, and non-Latin character comments, and empty lines: (41218, 3)\n",
      "                                              Comment  \\\n",
      "0     I love Pakistan Army from the core of my heart.   \n",
      "1                                Operation hinterland   \n",
      "2   Aik He Cherah hai jesay Dekh Muje Khud Bakhud ...   \n",
      "3                                     I love pak army   \n",
      "6   Nafrt ho gai hain ap sy. Kya bna dea Ic mulk P...   \n",
      "7          Israili army ke liye kam krti hai lol army   \n",
      "8                                  Qadyany--fuj-gulam   \n",
      "11  Masha ALLAH Masha ALLAH.......kitni achi army hai   \n",
      "12                          🇵🇰 Pakistan army Zindabad   \n",
      "13  Illegal activities are not allowed in Pakistan...   \n",
      "\n",
      "                                  Roman Urdu Comments Toxic comments  \n",
      "0                       love pakistan army core heart                 \n",
      "1                                operation hinterland                 \n",
      "2   aik cherah jesay dekh muje khud bakhud gussa a...                 \n",
      "3                                       love pak army                 \n",
      "6   nafrt gai sy bna dea ic mulk pakistan kaallah ...                 \n",
      "7                     israili army liye krti lol army                 \n",
      "8                                     qadyanyfujgulam                 \n",
      "11                  masha allah masha allahkitni army                 \n",
      "12                             pakistan army zindabad                 \n",
      "13      illegal activities allowed pakistan inshallah                 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape after removing duplicates, stopwords, English, and non-Latin character comments, and empty lines: {comments_df.shape}\")\n",
    "print(comments_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda0d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined comments saved successfully to 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/refined_youtube_comments.csv'.\n"
     ]
    }
   ],
   "source": [
    "refined_comments_df = comments_df.drop(columns=['Comment', 'Toxic comments'])\n",
    "\n",
    "# Saving the refined DataFrame to a new CSV file\n",
    "refined_output_file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/refined_youtube_comments.csv'\n",
    "refined_comments_df.to_csv(refined_output_file_path, index=False)\n",
    "\n",
    "print(f\"Refined comments saved successfully to '{refined_output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9e0a228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully with the toxic Roman Urdu words.\n"
     ]
    }
   ],
   "source": [
    "toxic_words = ['beghairat','nafarat','nafrt', 'chutiya', 'haramzada', 'harami', 'ghaleez', 'gandu', 'kameena', 'laanti', 'lanat', \n",
    "    'bhadwa', 'behnchod', 'madarchod', 'ullu ka pattha', 'kutta', 'kutti', 'bhenchod', 'bhosda', \n",
    "    'haramkhor', 'choot', 'jaahil', 'paagal', 'kuttiya', 'zaleel', 'ghatia', 'nikamma', 'namakharam', \n",
    "    'badtameez', 'ghatiya', 'besharam', 'bakwas', 'nalayak', 'fazool', 'zameerfarosh', 'badtameez', \n",
    "    'kutta kamina', 'jhootay', 'fraudia', 'bakwas', 'chichora', 'kanjar', 'bhikari', 'charsi', \n",
    "    'ghaleez', 'begumaat', 'haram ki aulad', 'jaahil log', 'tatti', 'chootia', 'haramzada', 'bhaand', \n",
    "    'chalu', 'chamaar', 'chamdi', 'dehaati', 'gawar', 'nalayak', 'paagal', 'pehngah', 'raand', \n",
    "    'rascal', 'randi', 'saala', 'sarkari kutta', 'tharki', 'chirkut', 'kachra', 'dalla', 'bhains', \n",
    "    'bachi', 'chalu aurat', 'chikna', 'choor', 'dheet', 'farzi', 'ghatiya', 'kanjar', 'katwa', \n",
    "    'khoon kharaba', 'khoon peena', 'kuttay', 'kutti', 'nalayak', 'nasli', 'neech', 'paapi', \n",
    "    'phuddu', 'phuski', 'rakhail', 'rasgulla', 'saand', 'sanki', 'sasti', 'thook', 'thukna', \n",
    "    'waarsi', 'zillat', 'zinda lash', 'zinda murda', 'zinda qabristan', 'zubaan', 'zuban darazi', \n",
    "    'zuban samajhna', 'zuban daraz', 'zinda laash', 'zyada dimagh na kharab kar', 'tumse kuch nahi hoga', \n",
    "    'chal nikal', 'mujhe bakwas mat suna', 'tere baap ka naukar nahi hoon', 'ghalat mat samajh', \n",
    "    'aurat kya samjhti hai apne aap ko', 'teri maa ka', 'tere baap ka', 'teri behen ka', 'teri biwi ka', \n",
    "    'tu kya samjhta hai', 'teri aukaat kya hai', 'chal hatt', 'chal bhag', 'bakwas kar raha hai', \n",
    "    'chor hai tu', 'dheela hai tu', 'tera dimagh kharab hai', 'tere dimagh mein kuch nahi', \n",
    "    'tere bas ki baat nahi', 'tu kuch nahi kar sakta', 'tera kuch nahi hoga', 'chal bhaag yahan se', \n",
    "    'chal nikal yahan se', 'chal tu apna kaam kar', 'chal apna raasta naap', 'tere liye yahaan kuch nahi hai', \n",
    "    'tere ko koi nahi puchhega', 'tera kuch nahi hone wala', 'chal apni shakal dekh', 'chal tu apna kaam kar', \n",
    "    'chal apni aukaat dekh', 'tu zindagi mein kuch nahi kar sakta', 'chal apni shakal dekh', 'chal tu apna kaam kar', \n",
    "    'chal tu kuch nahi kar sakta','Shaytans hugging','kusra','kadra shemale','kadra','shemale', 'chal tu zindagi mein kuch nahi kar sakta',\n",
    "]\n",
    "\n",
    "toxic_words_df = pd.DataFrame(toxic_words, columns=['Toxic_Roman_urdu_Words'])\n",
    "\n",
    "file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/Toxic_Roman_urdu_Words.csv'\n",
    "toxic_words_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"CSV file saved successfully with the toxic Roman Urdu words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24fcdd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated successfully with toxic Roman Urdu words.\n"
     ]
    }
   ],
   "source": [
    "# Load the toxic comments from the Excel sheet\n",
    "file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/Roman Urdu Toxic Dataset.xlsx'\n",
    "excel_data = pd.read_excel(file_path)\n",
    "\n",
    "# Filter comments marked as toxic\n",
    "toxic_comments_from_excel = excel_data[excel_data['toxic'] == 1]['Text'].tolist()\n",
    "\n",
    "# Combine the toxic comments with the existing toxic words list\n",
    "combined_toxic_words = toxic_words + toxic_comments_from_excel\n",
    "\n",
    "# Remove duplicates and refine the list\n",
    "combined_toxic_words = list(set(combined_toxic_words))\n",
    "\n",
    "# Convert the final list to a DataFrame\n",
    "toxic_words_df = pd.DataFrame(combined_toxic_words, columns=['Toxic_Roman_Urdu_Words'])\n",
    "\n",
    "# Save the updated dataset to a CSV file\n",
    "output_file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/Updated_Toxic_Roman_urdu_Words.csv'\n",
    "toxic_words_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"CSV file updated successfully with toxic Roman Urdu words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a129d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated comments saved successfully to 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/updated_youtube_comments.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load the updated toxic words list\n",
    "toxic_words_file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/Updated_Toxic_Roman_urdu_Words.csv'\n",
    "toxic_words_df = pd.read_csv(toxic_words_file_path)\n",
    "toxic_words_list = toxic_words_df['Toxic_Roman_Urdu_Words'].tolist()\n",
    "\n",
    "# Load the refined YouTube comments\n",
    "comments_file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/refined_youtube_comments.csv'\n",
    "comments_df = pd.read_csv(comments_file_path)\n",
    "\n",
    "# Function to check if any toxic words appear in the comment\n",
    "def is_toxic(comment):\n",
    "    for word in toxic_words_list:\n",
    "        if word in comment:\n",
    "            return 1  # Mark as toxic\n",
    "    return 0  # Mark as non-toxic\n",
    "\n",
    "# Apply the function to the comments\n",
    "comments_df['Toxic'] = comments_df['Roman Urdu Comments'].apply(is_toxic)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file_path = 'C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/updated_youtube_comments.csv'\n",
    "comments_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated comments saved successfully to '{output_file_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77489e",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485518db",
   "metadata": {},
   "source": [
    "Initialize the TF-IDF Vectorizer for Unigrams and Bigrams and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b73c08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(comments_df['Roman Urdu Comments'])\n",
    "y = comments_df['Toxic']  # The target variable is 'Toxic' (1 for toxic, 0 for non-toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414bff8",
   "metadata": {},
   "source": [
    "# 4. Train & Test Data Split (70% Training, 30% Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3dc45c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a48567",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8437b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models\n",
    "nb_model = MultinomialNB(class_prior=[0.3, 0.7])\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train Models\n",
    "nb_model.fit(X_train, y_train)\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27776136",
   "metadata": {},
   "source": [
    "# 6. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb8237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Evaluation\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "nb_confusion = confusion_matrix(y_test, nb_preds)\n",
    "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
    "nb_precision = precision_score(y_test, nb_preds, zero_division=1)\n",
    "nb_recall = recall_score(y_test, nb_preds, zero_division=1)\n",
    "nb_f1 = f1_score(y_test, nb_preds, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3324878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Evaluation\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "dt_confusion = confusion_matrix(y_test, dt_preds)\n",
    "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
    "dt_precision = precision_score(y_test, dt_preds, average='weighted')\n",
    "dt_recall = recall_score(y_test, dt_preds, average='weighted')\n",
    "dt_f1 = f1_score(y_test, dt_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af0e536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Evaluation\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_confusion = confusion_matrix(y_test, rf_preds)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "rf_precision = precision_score(y_test, rf_preds, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_preds, average='weighted')\n",
    "rf_f1 = f1_score(y_test, rf_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acdc2102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Confusion Matrix:\n",
      "[[9577 2619]\n",
      " [  37  133]]\n",
      "Naive Bayes Accuracy: 0.7852\n",
      "\n",
      "Naive Bayes Precision: 0.0483\n",
      "Naive Bayes Recall: 0.7824\n",
      "Naive Bayes F1-Score: 0.0910\n",
      "Decision Tree Confusion Matrix:\n",
      "[[12191     5]\n",
      " [   29   141]]\n",
      "Decision Tree Accuracy: 0.9973\n",
      "\n",
      "Decision Tree Precision: 0.9972\n",
      "Decision Tree Recall: 0.9973\n",
      "Decision Tree F1-Score: 0.9971\n",
      "Random Forest Confusion Matrix:\n",
      "[[12196     0]\n",
      " [  122    48]]\n",
      "Random Forest Accuracy: 0.9901\n",
      "\n",
      "Random Forest Precision: 0.9902\n",
      "Random Forest Recall: 0.9901\n",
      "Random Forest F1-Score: 0.9874\n"
     ]
    }
   ],
   "source": [
    "# 7. Print Evaluation Results\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(nb_confusion)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\\n\")\n",
    "print(f\"Naive Bayes Precision: {nb_precision:.4f}\")\n",
    "print(f\"Naive Bayes Recall: {nb_recall:.4f}\")\n",
    "print(f\"Naive Bayes F1-Score: {nb_f1:.4f}\")\n",
    "\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(dt_confusion)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\\n\")\n",
    "print(f\"Decision Tree Precision: {dt_precision:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1-Score: {dt_f1:.4f}\")\n",
    "\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(rf_confusion)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\\n\")\n",
    "print(f\"Random Forest Precision: {rf_precision:.4f}\")\n",
    "print(f\"Random Forest Recall: {rf_recall:.4f}\")\n",
    "print(f\"Random Forest F1-Score: {rf_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0f79a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1-Score  \\\n",
      "0    Naive Bayes  0.785218   0.048328  0.782353  0.091034   \n",
      "1  Decision Tree  0.997251   0.997189  0.997251  0.997147   \n",
      "2  Random Forest  0.990134   0.990232  0.990134  0.987398   \n",
      "\n",
      "            Confusion Matrix  \n",
      "0  [[9577, 2619], [37, 133]]  \n",
      "1    [[12191, 5], [29, 141]]  \n",
      "2    [[12196, 0], [122, 48]]  \n",
      "Model evaluation results saved to 'model_evaluation_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {\n",
    "    'Model': ['Naive Bayes', 'Decision Tree', 'Random Forest'],\n",
    "    'Accuracy': [nb_accuracy, dt_accuracy, rf_accuracy],\n",
    "    'Precision': [nb_precision, dt_precision, rf_precision],\n",
    "    'Recall': [nb_recall, dt_recall, rf_recall],\n",
    "    'F1-Score': [nb_f1, dt_f1, rf_f1],\n",
    "    'Confusion Matrix': [nb_confusion, dt_confusion, rf_confusion]\n",
    "}\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "print(evaluation_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "evaluation_df.to_csv('C:/Users/LENOVO/Desktop/BC200404125 FYP 2024/data/model_evaluation_results.csv', index=False)\n",
    "print(\"Model evaluation results saved to 'model_evaluation_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "990b8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e94de30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
    "\n",
    "\n",
    "joblib.dump(dt_model, 'decision_tree_model.pkl')\n",
    "\n",
    "\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd873e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf58f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/BC200404125 FYP 2024\\result\\main.html\"\n",
    "\n",
    "# Open the HTML file in the default web browser\n",
    "webbrowser.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ce222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
